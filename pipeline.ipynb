{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type 'Diabetes_012' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m X_train, X_holdout, y_train, y_holdout \u001b[38;5;241m=\u001b[39m train_test_split(X, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     36\u001b[0m numerical_cols \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnumber)\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()  \n\u001b[1;32m---> 37\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiabetes_012\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHighBP,HighChol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCholCheck\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBMI,Smoker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStroke\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeartDiseaseorAttack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPhysActivity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFruits,Veggies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHvyAlcoholConsump\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAnyHealthcare\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNoDocbcCost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGenHlth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMentHlth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPhysHlth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiffWalk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSex,Age\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEducation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIncome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()  \n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mColumnNameKeeper\u001b[39;00m(TransformerMixin):  \n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \n",
      "File \u001b[1;32mc:\\Users\\crumb\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\frame.py:4677\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   4674\u001b[0m             converted_dtypes\u001b[38;5;241m.\u001b[39mappend(infer_dtype_from_object(dtype))\n\u001b[0;32m   4675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfrozenset\u001b[39m(converted_dtypes)\n\u001b[1;32m-> 4677\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_int_infer_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4678\u001b[0m exclude \u001b[38;5;241m=\u001b[39m check_int_infer_dtype(exclude)\n\u001b[0;32m   4680\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dtypes \u001b[38;5;129;01min\u001b[39;00m (include, exclude):\n",
      "File \u001b[1;32mc:\\Users\\crumb\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\frame.py:4674\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes.<locals>.check_int_infer_dtype\u001b[1;34m(dtypes)\u001b[0m\n\u001b[0;32m   4672\u001b[0m         converted_dtypes\u001b[38;5;241m.\u001b[39mextend([np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32])\n\u001b[0;32m   4673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4674\u001b[0m         converted_dtypes\u001b[38;5;241m.\u001b[39mappend(\u001b[43minfer_dtype_from_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   4675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfrozenset\u001b[39m(converted_dtypes)\n",
      "File \u001b[1;32mc:\\Users\\crumb\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1602\u001b[0m, in \u001b[0;36minfer_dtype_from_object\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m   1594\u001b[0m         \u001b[38;5;66;03m# Handles cases like get_dtype(int) i.e.,\u001b[39;00m\n\u001b[0;32m   1595\u001b[0m         \u001b[38;5;66;03m# Python objects that are valid dtypes\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[38;5;66;03m# TypeError handles the float16 type code of 'e'\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m         \u001b[38;5;66;03m# further handle internal types\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m infer_dtype_from_object(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: data type 'Diabetes_012' not understood"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from typing import Tuple  \n",
    "from sklearn.impute import SimpleImputer    \n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.compose import ColumnTransformer    \n",
    "from sklearn.model_selection import train_test_split    \n",
    "from xgboost import XGBClassifier    \n",
    "import category_encoders as ce   \n",
    "from sklearn.base import TransformerMixin  \n",
    "from dataclasses import dataclass  \n",
    "from sklearn.base import BaseEstimator, ClassifierMixin      \n",
    "import gc  \n",
    "import xgboost as xgb    \n",
    "from sklearn.metrics import roc_curve  \n",
    "# Assume X and y are your features and target \n",
    "\n",
    "# Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# List of classifiers classifiers = [ ('lr', LogisticRegression(random_state=42)), ('svm', SVC(random_state=42)), ('rf', RandomForestClassifier(random_state=42, n_estimators=128)) ] \n",
    "\n",
    "# Define a VotingClassifier voting_clf = VotingClassifier(estimators=classifiers)\n",
    "#__author__ = 'Firas Obeid'\n",
    "# Setup X and y variables\n",
    "# X = df.drop(columns='y')\n",
    "# y = df['y'].values.reshape(-1,1)\n",
    "# y = np.where(y == 'no', 0, 1)\n",
    "df = pd.read_csv('datasets/diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop(columns=[\"Diabetes_binary\"])\n",
    "y = df[\"Diabetes_binary\"]\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, random_state=42)\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()  \n",
    "categorical_cols = X.select_dtypes(include=['Diabetes_012','HighBP,HighChol', 'CholCheck', 'BMI,Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits,Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex,Age', 'Education', 'Income']).columns.tolist()  \n",
    "\n",
    "class ColumnNameKeeper(TransformerMixin):  \n",
    "    def fit(self, X, y=None):  \n",
    "        return self  \n",
    "  \n",
    "    def transform(self, X):  \n",
    "        self.column_names = X.columns  \n",
    "        return X  \n",
    "    \n",
    "class NullImputa(TransformerMixin):      \n",
    "    def __init__(self, min_count_na = 5):      \n",
    "        self.min_count_na = min_count_na      \n",
    "        self.missing_cols = None    \n",
    "        self.additional_features = []  # Store names of additional features  \n",
    "        self.column_names = None  # Store column names after transformation  \n",
    "        \n",
    "    def fit(self, X, y=None):      \n",
    "        self.missing_cols = X.columns[X.isnull().any()].tolist()      \n",
    "        return self      \n",
    "        \n",
    "    def transform(self, X, y=None):      \n",
    "        X = X.copy()  # create a copy of the input DataFrame      \n",
    "        for col in X.columns.tolist():      \n",
    "            if col in X.columns[X.dtypes == object].tolist():      \n",
    "                X[col] = X[col].fillna(X[col].mode()[0])      \n",
    "            else:      \n",
    "                if col in self.missing_cols:      \n",
    "                    new_col_name = f'{col}-mi'  \n",
    "                    X[new_col_name] = X[col].isna().astype(int)     \n",
    "                    self.additional_features.append(new_col_name)  # Store the new column name  \n",
    "                if X[col].isnull().sum() <= self.min_count_na:      \n",
    "                   X[col] = X[col].fillna(X[col].median())      \n",
    "                else:      \n",
    "                    X[col] = X[col].fillna(-9999)      \n",
    "        assert X.isna().sum().sum() == 0      \n",
    "        _ = gc.collect()      \n",
    "        print(\"Imputation complete.....\") \n",
    "        self.column_names = X.columns.tolist()  # Store column names after transformation  \n",
    "        return X  \n",
    "\n",
    "    \n",
    "def ks_stat(y_pred, dtrain)-> Tuple[str, float]:\n",
    "    y_true = dtrain.get_label()  \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)  \n",
    "    ks_stat = max(tpr - fpr)  \n",
    "    return 'ks_stat', ks_stat \n",
    "\n",
    "class XGBoostClassifierWithEarlyStopping(BaseEstimator, ClassifierMixin):    \n",
    "    def __init__(self, nfolds=5, **params):    \n",
    "        self.params = params    \n",
    "        self.evals_result = {}    \n",
    "        self.bst = None  \n",
    "        self.nfolds = nfolds  \n",
    "        self.cvresult = None  # initialize cvresult attribute \n",
    "    \n",
    "    def fit(self, X, y, **fit_params):    \n",
    "        dtrain = xgb.DMatrix(X, label=y)  \n",
    "        self.cvresult = xgb.cv(self.params, dtrain, num_boost_round=10000,  verbose_eval=True, maximize=True,\n",
    "                          nfold=self.nfolds, metrics=['auc'], custom_metric = ks_stat,\n",
    "                          early_stopping_rounds=10, stratified=True,  \n",
    "                          seed=42)  \n",
    "        self.bst = xgb.train(self.params, dtrain, num_boost_round=self.cvresult.shape[0], feval = ks_stat)  \n",
    "        return self    \n",
    "    \n",
    "    def predict(self, X):    \n",
    "        dtest = xgb.DMatrix(X)    \n",
    "        return self.bst.predict(dtest)  \n",
    "\n",
    "  \n",
    "def init_model_params(y_train):\n",
    "    '''\n",
    "    “binary:logistic” –logistic regression for binary classification, output probability\n",
    "    “binary:logitraw” –logistic regression for binary classification, output score before logistic transformation\n",
    "    '''\n",
    "    #define class weight dictionary, negative class has 20x weight\n",
    "    # w = {0:20, 1:1}\n",
    "    global params\n",
    "    global plst\n",
    "    # plst = \n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    params = {\"booster\" :\"gbtree\",\n",
    "              \"max_depth\" : 5,\n",
    "              \"n_jobs\": -1,\n",
    "              \"verbosity\" : 3,\n",
    "             \"objective\": \"binary:logistic\",\n",
    "             \"eta\": 0.05,\n",
    "              \"colsample_bytree\" : 0.3, \n",
    "             \"tree_method\": \"exact\",\n",
    "             \"scale_pos_weight\": int((y_train.value_counts()[0] / y_train.value_counts()[1])),\n",
    "             \"eval_metric\": [\"auc\", \"logloss\", \"error\"],\n",
    "             \"subsample\" : 0.8, \"colsample_bylevel\" : 1, \"random_state\" : 42, \"verbosity\" : 3}   \n",
    "\n",
    "init_model_params(y_train)\n",
    "# Create a preprocessor for numerical columns  \n",
    "numeric_transformer = Pipeline(steps=[  \n",
    "    ('custome-imputer', NullImputa(5)),  \n",
    "    ('scaler', StandardScaler())])  \n",
    "\n",
    "# Create a preprocessor for categorical columns  \n",
    "categorical_transformer = Pipeline(steps=[  \n",
    "    ('custome-imputer', NullImputa(5)),  \n",
    "    ('target_encoder', ce.TargetEncoder())])  \n",
    " \n",
    "\n",
    "# Combine the preprocessors using a ColumnTransformer  \n",
    "preprocessor = ColumnTransformer(  \n",
    "    transformers=[  \n",
    "        ('num', numeric_transformer, numerical_cols),  \n",
    "        ('cat', categorical_transformer, categorical_cols)])  \n",
    "  \n",
    "# Create a pipeline that combines the preprocessor with the estimator  \n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),  \n",
    "                           ('classifier', XGBoostClassifierWithEarlyStopping(**params))])  \n",
    "  \n",
    "# Fit the pipeline to the training data  \n",
    "pipeline.fit(X_train, y_train)  \n",
    "# classifier__eval_set=[(pipeline.named_steps['preprocessor'].transform(X_test), y_test)])\n",
    "# Now you can use pipeline.predict() to make predictions on new data \n",
    "# pipeline.predict(X_holdout) \n",
    "\n",
    "cv_results = pipeline.named_steps['classifier'].cvresult\n",
    "cv_results[cv_results.columns[cv_results.columns.str.contains('ks.*mean|mean.*ks', regex=True)]].plot()\n",
    "\n",
    "y_train_pred = pipeline.predict(X_train) \n",
    "y_holdout_pred = pipeline.predict(X_holdout) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
